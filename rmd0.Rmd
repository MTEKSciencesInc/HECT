---
title: "Highly Efficient Clinial Trial simulator - HECT"
author: "MTEK Sciences\n"
output:
  html_document: default
  pdf_document: default
---
<style>
.nobullet li {
  list-style-type: none;
}
</style>


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

### Summary

The highly efficient clinical trial (HECT) simulator is an online-application that allows users to perform simulations of Bayesian platform adaptive trial to evaluate the pros and cons of candidate trial designs. They key design features in the application are early stopping for benefits (i.e. superiority) or lack of benefits (i.e. futility), as well as the flexibility of dropping or adding treatment arms. The application incorporates a wide range of other features to optimize efficiency and ethics of the clinical trial as, such as a number of time and cost related inputs and outputs. This application was created by MTEK Sciences Inc. with the support from the Bill and Melinda Gates Foundation. 


### Input panel

To perform simulations, the user is required to specify a design. The following design components are specified by the user in the input panel to the left side of the window:

**Number of treatments:** Number of treatment arms in the trial including the control arm.

**Type of primary outcome:** The user can choose either “Continuous” or “Proportion” as the type of outcome. Continuous outcomes (such as measurements of height or weight) need to be standardized to have mean 0 and variance 1. If “Proportion” is selected as the type of outcome, dichotomous patient responses are simulated. 

**Direction of effect:** The user can specify if a larger effect size is considered positive by choosing the “Increment/Event is favorable” option. More specifically, if “Increment/Event is favorable”, the treatment with the largest specified effect is assumed superior while if “Increment/Event is unfavorable”, the treatment with the smallest specified effect is assumed superior.

**Effect sizes/proportions:** The assumed effect size (standardized) for continuous outcomes and proportion of events (a value between 0 and 1) for dichotomous outcomes need to be specified by the user.

**Probabilities of adherence:** The default value for the probability of adherence is 1, i.e., it is assumed that for 100% of patients adheres to the treatment. The user may select a smaller value based on available prior knowledge for the probability of adherence to some or all the treatments. An adherence probability of 90% means that only 90% of the subjects that where assigned to a treatment arm actually received the treatment. The adherence can be different for each treatment. The probabilities of adherence are used in simulating the outcome but are ignored in any interim or final analysis, i.e., the trialist is assumed blinded to the fact that the adherence may be less than 100%. 

**Secondary outcome:** The user may choose to add a secondary outcome to be monitored. The secondary outcome is not used to inform any adaptations or stopping rules, but merely monitored. Setting a secondary outcome thereby allows the user to monitor what happens to power and type I error or detecting an effect for the secondary outcome under different designs and adaptations made based on the results of the primary outcome. The format for entering secondary outcomes the same as that of the primary outcomes.

**Platform design:** By default, a trial design begins randomization to all treatment arms with the first patient. A Platform design is one in which randomization to one or more treatments commences after some pre-determined point later during the trial. The HECT simulator provides an option for a simple platform design, in which one new treatment arm can be added upon the first instance where another treatment arm is dropped. In the HECT simulator, the treatment arm that is added later is be default set to the last treatment arm. For example, in a three arm trial where the platform design box has been checked, the addition of ‘Treatment 3’ is automatically triggered by the dropping of either ‘Treatment 1’ or ‘Treatment 2’. If no treatment is dropped before reaching the ‘Maximum total sample size’, ‘Treatment 3’ will not be added to the trial.  

**Comparisons:** Depending on the research question, the user may be interested in a design where all the arms are compared simultaneously, or a design where all arms are compared to one reference treatment. The stopping rules and adaptations are performed differently under these two designs.

<div class="nobullet">
* #### Compare all arms simultaneously

*   **Response adaptive randomization:** When all arms are compared simultaneously, the user may choose to adjust the allocation ratio to favour the treatment arm that appears to be performing better. The allocation ratio for each treatment arm is determined by the probability that the corresponding treatment is superior to the rest. If the response adaptive randomization option is selected, patients are allocated according to the ratio between the square roots of the probabilities of superiority across treatment arms. Note, the square root is used to stabilize allocation and thereby avoiding overt allocation to a single treatment. If the response adaptive randomization option is not selected the trial continues with equal allocation to all arms.

*   **Stopping arm for futility at:** Under the simultaneous comparison option futility for an arm is defined as the case where probability of the corresponding treatment being superior to all other treatments is negligible. More specifically an arm is dropped if probability of being superior among all treatments falls below some threshold. This lower threshold is specified by the user and the default value is set at 0.01 (1%). As such, with the default setting a treatment is dropped if the probability of it being superior to all other treatments falls below 1%. 

*   **Terminating trial for Superiority:** If the probability that a treatment arm is superior to all other treatments crosses an upper threshold the trial is terminated. This upper threshold is specified by the user and the default value is 0.975 (97.5%). As such, with the default setting the trial is terminated if the probability of any one treatment being superior to all others exceed 97.5%.

* #### Compare arms against reference treatment

*   **Minimally important difference:** When arms are compared to a reference treatment (by default set to ‘Treatment 1’), a minimally important difference (MID) may is used to determine futility for each pair wise comparison to the reference treatment. The default value for MID is set at 0 and can be specified to any value between 0 and 1. The MID is specified as an absolute value. For example, for a binary outcome an MID of 0.05 corresponds to a 5% absolute difference between the reference treatment and any other treatment. For a continuous outcome an MID of 0.2 corresponds to an absolute difference of 0.2 standard deviation units.

*   **Stopping and arm for futility:** Under the pairwise control comparison option futility for a treatment is defined as the case where the probability that the difference between the corresponding treatment and the reference treatment being smaller than the MID exceeds a certain upper threshold. This upper threshold is specified by the user and its default value is 0.95. Note that the reference treatment arm (first arm) is never dropped under the comparison to control option. As an example of this rule, if the outcome is binary, the MID is set to 5% (0.05 in the application) and the probability threshold at 95% (0.95 in the application), then the treatment being compared to the reference treatment is dropped at an interim look if the probability of the difference between the two treatments being smaller than 5% (0.05) exceeds 95%.

*   **Stopping for Superiority:** superiority stopping rule is the same as that used under the simultaneous comparison design.  
</div>


**Burn-in sample size:**
The user can specify an initial sample size that is required to be collected before any adaptation or stopping decision is made. That is, no possible adaptations (trial termination, dropping arms, adaptive allocation ratio, etc.) are made before the trial has reached its burn in sample size. The default value is set to 100 patients.

**Interim sample size:**
In the HECT simulator application, interim looks are equidistant by default. The interim sample size is the number of patients enrolled between each interim look. This number also indirectly determines the number of interim looks during the trial. Note, if the interim sample size is smaller than the burn-in sample size, the first interim look will be performed at a number of patients divisible by the interim sample size which is larger than the burn in sample size. Also note that the frequency of interim looks increases the simulation run time.

**Maximum sample size:**
The maximum permitted total number of patients. If the trial is not stopped according to the stopping rules before the maximum sample size is reached it will terminate when the sample size exceeds this number. 

**Enrollment cost:**
The user can specify the enrollment cost of each patient in dollar which allows for trial cost evaluation and comparisons.


### Trial design properties

Once all trial design inputs have been specified, simulations can be run. The trial design properties tab sets of the main panel (top right side of the screen) is where multiple simulations are run and simulation outputs are available. The statistical properties of the design are estimated by repeatedly simulating the trial and monitoring the frequency of a specific outputs (e.g., the number of times that the superior treatment is correctly concluded superior is the simulated power). The user can specify the number of simulations used. Increasing the number of simulations will increase the run time. Therefore, it is possible to define a limit for the run time. The run button to the right side of the panel initiates a simulation run. The simulation is terminated when the specified number of simulations have been completed or when the run time reaches this limit, whichever comes first. After a simulation run has been complete, the respective simulation outputs will be available under the tabs: ‘Power’, ‘Sample size distribution/cost evaluation’, and ‘Type I error rate’. The trial design properties panel also includes the ‘Saved simulation configurations and results’ tab, which allows the user to save simulation configurations and results from simulations runs under various configurations. Lastly, the ‘Comparison with RCT’ tab allows for a comparison between the platform/adaptive design and a conventional randomized clinical trial (RCT) design with respect to type I error, power, and cost.

**power:**
<div class="nobullet">
*   *Compare all arms simultaneously:* under this setting power is defined as the probability that the superior arm is correctly concluded superior at the end of the trial and is estimated as the proportion of times that this correct conclusion is made among the simulated trials.

*   *Compare arms against reference treatment:* under this setting power is defined for each arm (except control) as the probability that the corresponding treatment is concluded superior to control given that it is in fact superior to control.
</div>

**Sample size distribution/cost evaluation:** Under the adaptive design where stopping rules are used, the sample size at trial termination is not fixed. Therefore, one of the operational characteristics of any design with early stopping is the distribution of sample size at trial termination. A histogram of the final sample sizes for the simulated trials in addition to a table of quantiles of the distribution of final sample sizes is given under this tab. In addition, if cost per patient is specified by the user the same output is provided for the cost.

**Type I error rate:** 
<div class="nobullet">
*   *Compare all arms simultaneously:* under this setting type I error rate or probability of false positive is defined as the probability that any arm is concluded superior at the end of the trial given that all the treatment effects are the same (no effect). The effect sizes entered by the user are not used for estimating the false positive rate (type I error), the effect sizes are set to zero for continuous outcomes and the proportions are set the average of the values entered by the user. The false positive rate is estimated as the proportion of simulated trials where superiority is wrongly concluded.

*   *Compare arms against reference treatment:* under this setting type I error rate for each pairwise comparison is estimated as the proportion of simulated trials where the corresponding treatment is falsely concluded superior to control (i.e., under the scenario where the two do not differ). 
</div>

**Saved simulation configurations and results:** The user can manage the results of a completed simulation in this tab. The results can be temporarily saved for quick comparisons or be downloaded by the user. The user is allowed to configure the rows and columns of the table containing the simulation results to include the data that is of interest.

**Comparison with RCT:** It is often of interest to compare an adaptive design to a conventional design with fixed sample size. In the HECT simulator the properties of the design specified by the user are compared to a conventional design with fixed predetermined sample size, balanced randomization and no interim looks. The user has the option of fixing either sample size or power for the RCT design. If sample size is fixed (at maximum sample size), RCT power is calculated for the maximum sample size specified in the input panel and compared with simulated power for HECT. If power is fixed (at 80%), RCT sample size is calculated to acheive 80% power and is compared with expected sample size under a HECT design. The comparisons are presented in form of graphs of power, type I error rate and average cost for the two trial designs.

### Single Trial Simulation

To allow for further in-depth exploration of the behaviour under any given trial design, the HECT application also comes with an option to simulate one trial at a time. With a number of informative visuals, this allows the user to visually gauge just how much variation (certainty or uncertainty) one can expect from a single trial. For example, if the power is unexpectedly low, the user can monitor the probability of superiority for each treatment arm and the corresponding effect estimates for every interim look to understand the reason(s) behind low chance of detecting the signal. 

**Design:** A graphical representation of the trial is provided under the design tab that shows the portion of the trial duration that any arm was present in. If dropping of arms is part of the design, this output will show how long any treatment arm was kept in the trial for any single trial simulation. If a platform design is employed, this output will show when one arm was dropped and another added.

**Data: ** The data tab provides a visual summary of the patient data over time for the simulated trial. The plots represent observed patient outcomes (y-axis) split by treatment arm over time (x-axis). The response mean/proportion, allocation ratios and number of patients assigned to each arm are provided in a table below the graph.

**Probability of superiority:** The evolution of the probability of superiority of each treatment through the course of the trial can be explored. Since these probabilities are used for stopping decisions this tab provides explanations for the statistical properties estimated under the Trial design properties tab in terms of specific case scenarios. The probabilities of superiority are plotted for each interim look.	

**Estimates:** Point estimates and 95% credible intervals are provided for the parameters associated with each arm. These estimates may be used to obtain effect estimates in form of RR, OR and mean difference. The point estimates and 95% credible intervals are based on the full trial data set.

**Posterior:** The posterior distributions of the arm associated parameters at selected interim looks are provided to diagnose any estimation issues or provide explanations for the probability of superiority estimates that are generated from these posterior distributions.

**Secondary analysis:** If secondary outcomes are set to be monitored by the user, this tab will produce a graph of data over time similar to the one in the ‘Data’ tab as well as point estimates and 95% credible intervals similar to those in the ‘Estimates’ tab.


#### Technical details

The full technical details are available in the appendix. 


####Acknowledgements
The development of this application was funded by the Bill & Melinda Gates Foundation.

______________________________________________________________________________________________________
## Appendix -- Technical reference

### Trial Simulation

#### Bayesian response adaptive randomized design

BRARs are flexible trial designs that allow for adjustments to arm assignment ratios based on the intermediate results. Early termination of the trial or stopping an arm is allowed if there is "sufficient evidence" in the collected data that a treatment is outperforming the rest or is futile respectively. In both cases, "sufficient evidence" is defined during trial planning. The probability that a treatment effect is larger than all the other treatment effects given the observed data is often used as a measure of evidence for superiority or inferiority -- if this probability (or a function of it) crosses a pre-specified upper threshold for any of the treatment arms the trial is terminated; if it crosses a prespecified lower threshold for any of the treatments the corresponding arm is stopped, i.e. no more patients will be assigned to the stopped treatment arm. In case that all the treatments compete closely and none of the treatments are superior with probability greater than the specified upper threshold, the trial is terminated when a maximum sample size is reached. This maximum sample size is specified by the user as the maximum affordable sample size given the trial budget or based on an equivalent multi-arm conventional randomized clinical trial (RCT) with user-specified statistical properties. The `Sample Size Calculation` tab can be used for the latter.

For more on Response adaptive randomized designs and Bayesian adaptive trials see @RRABook and @BerryBook.

#### Data generative model
Consider a trial with $L$ arms. For every $i$th patient enrolled into the trial, let $\mathbf{x}_i$ be a vector of size $L$ whose $l^\text{th}$ component is 1 if treatment $l$ is assigned to patient $i$ and zero, otherwise. The outcome for patient $i$ is denoted by $y_i$ and the effect size of treatment $l$ is denoted by $\theta_l$. The vector of effect sizes, $\boldsymbol{\theta}$ is of size $L$. 

If the response type is continuous, the effect types are considered continuous and a normal model is used to generate data:
$$y_i \sim \mathcal{N}(\mathbf{x}_i^T\boldsymbol{\theta},1),$$
where $\mathbf{x}_i^T$ is the transpose of arm assignment vector.

If response type is a proportion, i.e. the individual outcomes are binary, the effect types are considered as the population proportion and are in the $(0,1)$ interval. A binomial model is used to generate the data in this case:
$$y_i \sim \mathcal{B}(1, \mathbf{x}_i^T\boldsymbol{\theta}).$$

The arm assignment vector, $\mathbf{x}_{i}$, is generated from the following multinomial distribution:
$$\mathbf{x}_i \sim \mathcal{M}(1, \boldsymbol{\rho} = (\rho_1, \ldots, \rho_L)),$$
where $\rho_l$ is the probability of assigning a patient to arm $l$. At the beginning of the trial and up until the $n^\text{th}$ patient, where $n$ is the burn-in sample size, patients are assigned to treatment arms using a balanced randomization scheme. That is, 
$$\rho_1 = \ldots = \rho_L = \frac{1}{L}.$$
However, after the burn-in period the arm assignment probabilities are adjusted according to the posterior probability that each arm is outperforming the rest given the cumulating data, i.e. the arm assignment probability for patient $n+1$ is specified as,
$$\rho_l=\sqrt{P\left(\theta_l= \max(\theta_1, \ldots, \theta_L) \mid \mathbf{x}_{1:n}, y_{1:n}\right)},$$
where $\mathbf{x}_{1:n}$ and $y_{1:n}$ are the "observed" assignment vectors and outcomes of the first $n$ patients enrolled in the trial. In the next section, we explain how $P\left(\theta_l= \max(\theta_1, \ldots, \theta_L) \mid \mathbf{x}_{1:n}, y_{1:n}\right)$ is estimated.

To incorporate the adherence rate into the data generating process, if the $l^{\text{th}}$ component of $\mathbf{x}_i$ is 1, it is set to zero with probability $\lambda_l$, where $\lambda_l$ is the adherence rate for arm $l$. Note that adherence is considered "unobservable", i.e. it is incorporated into the data generative model but not in the analysis model explained below.

#### Choosing the test hypotheses
There are two comparison options available and thus two sets of hypotheses that may be tested:

1. Compare all treatment arms simultaneously:
     $$H_0:\theta_i=\theta_j\; \text{for all}\; i,j \in \left\{ 1, 2, ..., L \right\},$$
     $$H_1:\; \text{There exists}\; i, j\; \text{such that}\; \theta_i \ne \theta_j$$
     
2. Compare each treatment arm against the reference treatment (Treatment 1).
    For each $i = \left\{ 1, 2, ..., L \right\}$,
     $$H_0: \theta_1 = \theta_i$$ 
     $$H_1: \theta_1 \ne \theta_i$$

#### Analysis model -- Bayesian updates
Each $\theta_l$ is analysed ``seperately'' in the sense that for each posterior distribution of $\theta_l$, only the relevant data for the $l$th arm is used. That is, define

* $y_{n_i:n_j,l}$: the responses from the group of $n_i$ to $n_j$ patients that were assigned the $l$th arm; all responses are assumed to be independent of one another.
* $n_{j,l}$: the number of patients out of $n_j$ total patients assigned to the $l$th arm (i.e, $\sum_{l=1}^{L}n_{j,l} = n_j$).

The interim Bayesian updates required for adaptation are performed as follows:

For continuous $\theta_l$, a diffuse normal prior is used,
$$\theta_l \sim \mathcal{N}(0,10).$$
The posterior can then be updated analytically; suppose that the posterior distribution of $\theta_l$ given the first $n_1$ patient data is,
$$\theta_l \mid \mathbf{x}_{1:n_1}, y_{1:n_1}\sim \mathcal{N}(\mu_1, \sigma_1^2).$$
The updated posterior according to $n_2$ new patients with assignment vectors $\mathbf{x}_{n_1:n_2}$ and outcomes $y_{n_1:n_2}$, is given by
$$\theta_l \mid \mathbf{x}_{1:(n_1+n_2)}, y_{1:(n_1+n_2)}\sim \mathcal{N}(\mu_2, \sigma_2^2),$$
where 
$$\sigma^2_2 = \frac{1}{n_{2,l} + \frac{1}{\sigma_1^2}},$$
and 
$$\mu_2 = \sigma_2^2 \left(\frac{\mu_1}{\sigma^2_1}+n_{2,l}\bar{y}_{n_1:n_2,l}\right),$$
where $\bar{y}_{n_1:n_2,l}$ is the mean response for patients who were assigned to arm $l$ among the new $n_2$ patients.

For rate type $\theta_l$,
$$\theta_l \sim Beta(1,1).$$
The posterior is obtained as follows; suppose that the posterior distribution of $\theta_l$ given the first $n_1$ patient data is,
$$\theta_l \mid \mathbf{x}_{1:n_1}, y_{1:n_1}\sim Beta(a_1, b_1).$$
The updated posterior according to $n_2$ new patients with assignment vectors $\mathbf{x}_{n_1:n_2}$ and outcomes $y_{n_1:n_2}$, is given by
$$\theta_l \mid \mathbf{x}_{1:(n_1+n_2)}, y_{1:(n_1+n_2)}\sim Beta(a_1 + n_{2,l}\bar{y}_{n_1:n_2,l}, b_1 + n_{2,l} - n_{2,l}\bar{y}_{n_1:n_2,l}),$$
where $n_{2,l}\bar{y}_{n_1:n_2,l}$ is the total number of successes in arm $l$ among the new $n_2$ patients.

The posterior probability of an arm being superior after $n$ patients are enrolled in the trial is then given as the expectation of $\mathbf{1}(\theta_l= \max(\theta_1, \ldots, \theta_L))$ with respect to the posterior $\pi(\theta_l \mid \mathbf{x}_{1:n}, y_{1:n})$,
$$P\left(\theta_l= \max(\theta_1, \ldots, \theta_L) \mid \mathbf{x}_{1:n}, y_{1:n}\right) = \int \mathbf{1}(\theta_l= \max(\theta_1, \ldots, \theta_L))\pi(\theta_l \mid \mathbf{x}_{1:n}, y_{1:n})d\theta_l,$$
where $\mathbf{1}(\theta_l= \max(\theta_1, \ldots, \theta_L))$ is an indicator function that takes the value of 1 if $\theta_l= \max(\theta_1, \ldots, \theta_L)$ and is zero otherwise. The probability of superiority is estimated using Monte Carlo. That is, for $N$ samples generated from $\pi(\theta_l \mid \mathbf{x}_{1:n}, y_{1:n})$ for $l = 1, \ldots, L$ the probabilities of superiority are estimated as
$$\hat{P}\left(\theta_l= \max(\theta_1, \ldots, \theta_L) \mid \mathbf{x}_{1:n}, y_{1:n}\right) = \sum_{i = 1}^N \mathbf{1}(\theta_{l,i}= \max(\theta_{1,i}, \ldots, \theta_{L,i})).$$
**Secondary outcome:** Addition of a secondary outcome is made possible in the trial simulator to allow for monitoring of the treatment effects on two outcomes in one trial. Note however that adaptations are performed only with respect to the primary outcome data. 

#### Trial design properties
In the `Trial design properties` tab, the properties of the study design such as power, type I error rate and the distribution of the "sample size at trial termination" may be studied. The trial with a set of user-specified settings is simulated mutiple times where the number of simulations is also specified by the user in the `power` tab.

**Power:** Statistical power for a given Bayesian response adaptive design is defined as the probability that the superior treatment is concluded superior given the data. Since the superiority conclusion is made if probability of superiority crosses the prespecified upper threshold, statistical power is defined as,
$$P\left(P\left(\theta_{max}= \max(\theta_1, \ldots, \theta_L) \mid \mathbf{x}_{1:n_T}, y_{1:n_T}\right)>U \mid \boldsymbol{\theta}, \text{design} \right).$$
Power is estimated as the proportion of times that the superior treatment is concluded superior in $M$ simulated trials given a set of effect sizes and design components,
$$\hat{P}\left(P\left(\theta_{\max}= \max(\theta_1, \ldots, \theta_L) \right)>U\right) = \frac{1}{M}\sum_{m = 1}^{M}\mathbf{1}\left(\hat{P}_m\left(\theta_{\max}= \max(\theta_1, \ldots, \theta_L)\right)>U\right),$$
where the subscript $m$ shows that the estimate was obtained from the data simulated in trial $m= 1, \ldots, M$.

**Sample size distribution:** The sample size at trial termination is an important criterion for adaptive designs. The distribution of the sample size at trial termination has the following relationship with the power: if the statistical power is high the trial is often concluded early and the sample size distribution has a mode around the burn-in sample size; if power is low, the trial more often reaches the maximum sample size without concluding and the sample size takes its mode at the maximum allowed sample size. A criterion that may be of interest is the expected saved sample size, which is the expected difference between the maximuam allowed sample size and the sample size at trial termination,
$$\hat{E}(SS) = \sum_{m = 1}^M(n_{\max} - n_T^m).$$
**Type I error rate:** The type I error rate is defined as the probability of concluding that one of the treatments is superior under the assumption that none of the treatments are effective. Type I error rate is estimated as,
$$\hat{P}\left(\exists l\hskip5pt \text{s.t.}\hskip5pt P\left(\theta_{l}= \max(\theta_1, \ldots, \theta_L) \right)>U \mid \theta_l = 0\hskip5pt \forall l\right) = \frac{1}{M}\sum_{m = 1}^{M}\mathbf{1}\left(\exists l\hskip5pt \text{s.t.}\hskip5pt P\left(\theta_{l}= \max(\theta_1, \ldots, \theta_L) \right)>U \mid \theta_l = 0\hskip5pt \forall l\right).$$  

### Sample Size Calculation
In the `Sample Size Calculation` tab, per-arm and total sample sizes can be calculated for a conventional multi-arm RCT. The goal is to obtain the required sample size to estimate the difference between the largest and second largest of the effects for a given statistical power, type I error rate, and drop-out rate. The rational is that in a multi-arm trial, the trial needs to be powered to detect the smallest difference between the largest of the effects and the rest.

If the continuous response type is selected, variances need to be provided. The sample size (per arm) is then given by 
$$n_a = \left\lceil\frac{(\sigma^2_1 + \sigma^2_2)(Z_{1-\alpha/2}+Z_{1-\beta})^2}{\delta^2(1-r)} \right\rceil $$
where $\delta$ is the difference between the largest effect size and the second largest effect size, $\sigma^2_1$ and $\sigma^2_2$ are the corresponding variances, $\alpha$ is the type I error rate and $\beta$ is $(1-\text{power})$, and $r$ is the dropout rate. 

If the proportion response type is selected, the sample size (per arm) is given by 
$$n_a = \left\lceil\frac{(p_1(1-p_1) + p_2(1-p_2))(Z_{1-\alpha/2}+Z_{1-\beta})^2}{(p_1 - p_2)^2(1-r)} \right\rceil $$
where $p_1$ and $p_2$ are the probabilities of success for the best and second best treatments.

The total sample size is simply calculated by multiplying the per-arm sample size by the number of trial arms.

####Acknowledgements
The development of this application was funded by the Bill & Melinda Gates Foundation.

#####References
<script type="text/x-mathjax-config">
   MathJax.Hub.Config({  "HTML-CSS": { minScaleAdjust: 125, availableFonts: [] }  });
</script>


---
references:
- id: RRABook
  title: Randomized Response Adaptive Designs in Clinical Trials
  author:
  - family: Atkinson
    given: Anthony C.
  - family: Biswas
    given: Atanu
  publisher: CRC Press
  type: Book
  issued:
    year: 2014
    
- id: BerryBook
  title: Bayesian Adaptive Methods for Clinical Trials
  author:
  - family: Berry
    given: Scott M.
  - family: Carlin
    given: Bradley P.
  - family: Lee
    given: J. Jack
  - family: Muller
    given: Peter
  publisher: CRC Press
  type: Book
  issued:
    year: 2010
---
